{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High level approach overview\n",
    "\n",
    "1. Data Preparation:\n",
    "    * Load the historical data\n",
    "    * Convert the date column to datetime format and set it as index\n",
    "2. Forecasting Algorithm:\n",
    "    * Exponential Smoothing: Used for capturing trends and seasonality in the data.\n",
    "    * ARIMA: Applied for modeling and forecasting time series data.\n",
    "    * Prophet: Designed for handling time series data with seasonal effects and missing data.\n",
    "3. Comparison:\n",
    "    * Forecasted CPU utilization for each service for the next 15 days using the three algorithms.\n",
    "    * Compare the results based on Root Mean Squared Error(RMSE) to determine the best algorithm for each service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pip install pandas \\\n",
    "> pip install numpy \\\n",
    "> pip install statsmodels \\\n",
    "> pip install scikit-learn \\\n",
    "> pip install matplotlib \\\n",
    "> pip install prophet \\\n",
    "> pip install seaborn \\\n",
    "> pip install tbats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "#from fbprophet import Prophet\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_csv(\"test-1_cpu_usage_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the date column to datetime format\n",
    "data['date'] = pd.to_datetime(data['date'], format=\"%d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>namespace</th>\n",
       "      <th>pods</th>\n",
       "      <th>cpu requested</th>\n",
       "      <th>cpu used</th>\n",
       "      <th>percentage of usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-09-07</td>\n",
       "      <td>test-1</td>\n",
       "      <td>54</td>\n",
       "      <td>258.6</td>\n",
       "      <td>91.9</td>\n",
       "      <td>35.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-09-08</td>\n",
       "      <td>test-1</td>\n",
       "      <td>177</td>\n",
       "      <td>606.0</td>\n",
       "      <td>245.7</td>\n",
       "      <td>40.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-09-09</td>\n",
       "      <td>test-1</td>\n",
       "      <td>132</td>\n",
       "      <td>634.1</td>\n",
       "      <td>151.2</td>\n",
       "      <td>23.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-09-10</td>\n",
       "      <td>test-1</td>\n",
       "      <td>108</td>\n",
       "      <td>482.3</td>\n",
       "      <td>144.2</td>\n",
       "      <td>29.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-09-11</td>\n",
       "      <td>test-1</td>\n",
       "      <td>62</td>\n",
       "      <td>250.1</td>\n",
       "      <td>58.9</td>\n",
       "      <td>23.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date namespace  pods  cpu requested  cpu used  percentage of usage\n",
       "0 2024-09-07    test-1    54          258.6      91.9                35.54\n",
       "1 2024-09-08    test-1   177          606.0     245.7                40.54\n",
       "2 2024-09-09    test-1   132          634.1     151.2                23.84\n",
       "3 2024-09-10    test-1   108          482.3     144.2                29.90\n",
       "4 2024-09-11    test-1    62          250.1      58.9                23.55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of list vectors must match length of `data` when both are used, but `data` has length 181 and the vector passed to `hue` has length 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu requested\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu used\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiple\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdodge\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshrink\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m.8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\distributions.py:1379\u001b[39m, in \u001b[36mhistplot\u001b[39m\u001b[34m(data, x, y, hue, weights, stat, bins, binwidth, binrange, discrete, cumulative, common_bins, common_norm, multiple, element, fill, shrink, kde, kde_kws, line_kws, thresh, pthresh, pmax, cbar, cbar_ax, cbar_kws, palette, hue_order, hue_norm, color, log_scale, legend, ax, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhistplot\u001b[39m(\n\u001b[32m   1359\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m, *,\n\u001b[32m   1360\u001b[39m     \u001b[38;5;66;03m# Vector variables\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m     **kwargs,\n\u001b[32m   1377\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     p = \u001b[43m_DistributionPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1384\u001b[39m     p.map_hue(palette=palette, order=hue_order, norm=hue_norm)\n\u001b[32m   1386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\distributions.py:110\u001b[39m, in \u001b[36m_DistributionPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    106\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    107\u001b[39m     variables={},\n\u001b[32m    108\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[39m, in \u001b[36mVectorPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m._var_ordered = {\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[39m, in \u001b[36mVectorPlotter.assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_format = \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     plot_data = \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     frame = plot_data.frame\n\u001b[32m    681\u001b[39m     names = plot_data.names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[39m, in \u001b[36mPlotData.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     data: DataSource,\n\u001b[32m     54\u001b[39m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[32m     55\u001b[39m ):\n\u001b[32m     57\u001b[39m     data = handle_data_source(data)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     frame, names, ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.names = names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Uttam Kumar\\Learning\\Time-series\\codebase\\venv\\Lib\\site-packages\\seaborn\\_core\\data.py:251\u001b[39m, in \u001b[36mPlotData._assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    245\u001b[39m         val_cls = val.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m    246\u001b[39m         err = (\n\u001b[32m    247\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLength of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_cls\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m vectors must match length of `data`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m when both are used, but `data` has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m and the vector passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    253\u001b[39m plot_data[key] = val\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# Try to infer the original name using pandas-like metadata\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Length of list vectors must match length of `data` when both are used, but `data` has length 181 and the vector passed to `hue` has length 2."
     ]
    }
   ],
   "source": [
    "#sns.histplot(data=data, x='date', hue=['cpu requested', 'cpu used'], multiple=\"dodge\", shrink=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the date column as index\n",
    "data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to forecast using Exponential Smoothing\n",
    "def forecast_exponential_smoothing(data, periods=15):\n",
    "    model = ExponentialSmoothing(data, trend='add', seasonal='add', seasonal_periods=12)\n",
    "    fit = model.fit()\n",
    "    forecast = fit.forecast(periods)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to forecast using ARIMA\n",
    "def forecast_arima(data, periods=15):\n",
    "    model = ARIMA(data, order=(5,1,0))\n",
    "    fit = model.fit()\n",
    "    forecast = fit.forecast(steps = periods)\n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to forecast using Prophet\n",
    "def forecast_prophet(data, periods=15):\n",
    "    df = data.reset_index().rename(columns ={'date': 'ds', 'value': 'y'})\n",
    "    model = Prophet()\n",
    "    model.fit(df)\n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    return forecast.set_index('ds')['yhat'][-periods:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecast for each service\n",
    "services = data['namespace'].unique()\n",
    "results = {}\n",
    "\n",
    "for service in services:\n",
    "    service_data = data[data['namespace'] == service]['cpu used']\n",
    "    \n",
    "    #Forecast using different algorithms\n",
    "    es_forecast = forecast_exponential_smoothing(service_data)\n",
    "    arima_forecast = forecast_arima(service_data)\n",
    "    prophet_forecast = forecast_prophet(service_data)\n",
    "    \n",
    "    #store the results\n",
    "    results[service] = {\n",
    "        'Exponential Smoothing': es_forecast,\n",
    "        'ARIMA': arima_forecast,\n",
    "        'Prophet': prophet_forecast\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results for comparison\n",
    "for service in services:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(data[data['service'] == service]['cpu used'], label='Historical Data')\n",
    "    plt.plot(results[service]['Exponential Smoothing'], label='Exponential Smoothing Forecast')\n",
    "    plt.plot(results[service]['ARIMA'], label='ARIMA Forecast')\n",
    "    plt.plot(results[service]['Prophet'], label='Prophet Forecast')\n",
    "    plt.title(f'CPU Utilization Forecast for {service}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the best algorithm based on RMSE\n",
    "best_algorithm = {}\n",
    "\n",
    "for service in services:\n",
    "    service_data = data[data['namespace'] == service]['cpu used']\n",
    "    \n",
    "    es_rmse = np.sqrt(mean_squared_error(service_data[-15], results[service]['Exponential Smoothing']))\n",
    "    arima_rmse = np.sqrt(mean_squared_error(service_data[-15], results[service]['ARIMA']))\n",
    "    prophet_rmse = np.sqrt(mean_squared_error(service_data[-15], results[service]['Prophet']))\n",
    "    \n",
    "    best_algorithm = min(\n",
    "        [('Exponential Smoothing', es_rmse), ('ARIMA', arima_rmse), ('Prophet', prophet_rmse)],\n",
    "        key = lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    best_algorithm[service] = best_algorithm\n",
    "    \n",
    "print(\"Best Algorithm for each service: \")\n",
    "for service, algorithm in best_algorithm.items():\n",
    "    print(f\"{service}: {algorithm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling new services without historical data can be challenging but there are several stratigies to make reasonable forecasts:\n",
    "\n",
    "1. Use Similar Services: Identify services that have similar characteristics or usage patterns to the new service. You can use their historical data as a proxy to estimate the resource utilization for new service.\n",
    "2. Benchmarking: Establish benchmarks based on industry standards or similar services. This can provide a starting point for resource allocation.\n",
    "3. Initial Allocation and Monitoring: Allocate resources based on initial estimates and closely monitor the utilization. Adjust the resources dynamically based on the observed usage patterns.\n",
    "4. Machine Learning Models: Use machine learning models that can handle cold-start problems. For example, collaborative filtering techniques can help predict resource utilization based on similarities between services.\n",
    "5. Expert Judgement: Consult with domain experts who have experience with similar services. Their insights can help in making informed estimates.\n",
    "6. Hybrid Approach: Combine multiple methods to improve the accuracy of your forecasts. For example, you can start with benchmarks and adjust based on real-time monitoring and expert judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
