{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model pipeline for time series forecasting using TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pip install tensorflow numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for evaluation\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM\n",
    "def prepare_data(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main LSTM pipeline\n",
    "def lstm_forecast(data, n_steps=30, n_epochs=50, batch_size=16):\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare training data\n",
    "    X, y = prepare_data(data_scaled, n_steps)\n",
    "    X_train, y_train = X[:-15], y[:-15]\n",
    "    X_test, y_test = X[-15:], y[-15:]\n",
    "\n",
    "    # Reshape data for LSTM (samples, timesteps, features)\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.1, verbose=1)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = scaler.inverse_transform(predictions)  # Inverse scale\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))  # Inverse scale\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(y_test, predictions)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test, label=\"Actual\", color=\"blue\")\n",
    "    plt.plot(predictions, label=\"Predicted\", color=\"orange\")\n",
    "    plt.title(\"LSTM Forecast vs Actual\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Forecast future steps\n",
    "    future_input = data_scaled[-n_steps:]\n",
    "    future_input = future_input.reshape((1, n_steps, 1))\n",
    "    future_forecast = model.predict(future_input)\n",
    "    future_forecast = scaler.inverse_transform(future_forecast)  # Inverse scale\n",
    "\n",
    "    print(f\"Future forecast (next step): {future_forecast.flatten()[0]:.3f}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    data = pd.read_csv(\"cpu_utilization_sample_data.tsv\", sep=\"\\t\")\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "    data = data.set_index(\"date\")\n",
    "    data = data[data[\"namespace\"] == \"test-1\"][\"cpu_used\"]  # Filter namespace\n",
    "\n",
    "    # Apply the LSTM pipeline\n",
    "    lstm_forecast(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "1. Data Preparation:\n",
    "\n",
    "    * Scaling: The data is normalized using MinMaxScaler to fit the input range for LSTM.\n",
    "    * Sequence Creation: The prepare_data function creates sequences of size n_steps for LSTM training.\n",
    "\n",
    "2. Model Architecture:\n",
    "\n",
    "    * The LSTM model has:\n",
    "        * Two LSTM layers (one with return_sequences=True to stack them).\n",
    "        * Two Dropout layers to prevent overfitting.\n",
    "        * A Dense layer to output the next time step value.\n",
    "\n",
    "3. Training:\n",
    "\n",
    "    * The model is trained with a specified number of epochs and batch size.\n",
    "    * The training process includes validation to monitor performance.\n",
    "\n",
    "4. Evaluation:\n",
    "\n",
    "    * Predictions are evaluated using MAE and RMSE.\n",
    "    * A plot compares actual and predicted values.\n",
    "\n",
    "5. Forecasting:\n",
    "\n",
    "    * The model predicts the next time step based on the last n_steps of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Notes\n",
    "* Parameter Tuning: You can adjust n_steps, n_epochs, and batch_size to optimize model performance for your dataset.\n",
    "\n",
    "* Future Forecasting: Extend future_forecast for multiple time steps by feeding predictions back into the model.\n",
    "\n",
    "* Scaling: Proper scaling and inverse scaling are crucial for accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
